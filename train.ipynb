{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import datasets as datasets\n",
    "from torchvision import transforms as transforms\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN():\n",
    "    def create_dataloader(self, name, train_batch, test_batch):\n",
    "        path = \"~/binarized-neural-network/\"\n",
    "\n",
    "        if name == \"MNIST\":\n",
    "            directory = path + \"mnist/\"\n",
    "\n",
    "            train = datasets.MNIST(root=directory, train=True, download=True, \n",
    "                                   transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                                                 transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "            test = datasets.MNIST(root=directory, train=False,\n",
    "                                  transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                  transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "            train_loader = DataLoader(train, batch_size=train_batch, \n",
    "                                      shuffle=True, pin_memory=True, num_workers=1)\n",
    "            test_loader = DataLoader(test, batch_size=test_batch,\n",
    "                                     shuffle=True, pin_memory=True, num_workers=1)\n",
    "\n",
    "        return train, test, train_loader, test_loader\n",
    "    \n",
    "    def create_model(self, name):\n",
    "        if name == \"MNIST\":\n",
    "            net = MNIST_BNN().to(self.device)\n",
    "        \n",
    "        return net\n",
    "\n",
    "    def __init__(self, name, output_dir, train_batch=100, test_batch=1000, num_epochs=10, lr=1e-3):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.train_set, self.test_set, self.train_loader, self.test_loader = self.create_dataloader(name, \n",
    "                                                                                                    train_batch, \n",
    "                                                                                                    test_batch)\n",
    "        self.net = self.create_model(name)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        \n",
    "        self.epochs = num_epochs\n",
    "        self.train_batch = train_batch\n",
    "        self.test_batch = test_batch\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.history = []\n",
    "        self.train_loss = []\n",
    "        self.train_acc = []\n",
    "        self.test_loss = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(output_dir, \n",
    "                                       \"checkpoint.pth.tar\")\n",
    "        self.config_path = os.path.join(output_dir, \"config.txt\")\n",
    "        \n",
    "        # Transfer all local arguments/variables into attributes\n",
    "        locs = {k: v for k, v in locals().items() if k is not 'self'}\n",
    "        self.__dict__.update(locs)\n",
    "        \n",
    "        if os.path.isfile(self.config_path):\n",
    "            with open(self.config_path, 'r') as f:\n",
    "                if f.read()[:-1] != repr(self):\n",
    "                    raise ValueError(\n",
    "                        \"Cannot create this experiment: \"\n",
    "                        \"I found a checkpoint conflicting with the current setting.\")\n",
    "            self.load()\n",
    "        else:\n",
    "            self.save()\n",
    "            \n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return len(self.history)\n",
    "    \n",
    "    def setting(self):\n",
    "        return {'Model': self.net,\n",
    "                'Optimizer': self.optimizer,\n",
    "                'TrainSet' : self.train_set,\n",
    "                'TestSet' : self.test_set,\n",
    "                'BatchSize': self.train_batch}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for key, val in self.setting().items():\n",
    "            string += '{}({})\\n'.format(key, val)\n",
    "        return string\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the current state of the experiment.\"\"\"\n",
    "        return {'Model': self.net.state_dict(),\n",
    "                'Optimizer': self.optimizer.state_dict(),\n",
    "                'History': self.history,\n",
    "                'TrainLoss' : self.train_loss,\n",
    "                'TrainAcc' : self.train_acc,\n",
    "                'TestLoss' : self.test_loss,\n",
    "                'TestAcc' : self.test_acc}\n",
    "    \n",
    "    def load_state_dict(self, checkpoint):\n",
    "        # load from pickled checkpoint\n",
    "        self.net.load_state_dict(checkpoint['Model'])\n",
    "        self.optimizer.load_state_dict(checkpoint['Optimizer'])\n",
    "        self.history = checkpoint['History']\n",
    "        self.train_loss = checkpoint['TrainLoss']\n",
    "        self.train_acc = checkpoint['TrainAcc']\n",
    "        self.test_loss = checkpoint['TestLoss']\n",
    "        self.test_acc = checkpoint['TestAcc']\n",
    "        \n",
    "        for state in self.optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(self.device)\n",
    "    def save(self):\n",
    "        ''''Saves the experiment on disk, i.e, create/update the last checkpoint.'''        \n",
    "        torch.save(self.state_dict(), self.checkpoint_path)\n",
    "        with open(self.config_path, 'w') as f:\n",
    "            print(self, file=f)  \n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Loads the experiment from the last checkpoint saved on disk.\"\"\"\n",
    "        checkpoint = torch.load(self.checkpoint_path,\n",
    "                                map_location=self.device)\n",
    "        self.load_state_dict(checkpoint)\n",
    "        del checkpoint\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.net.eval()\n",
    "        \n",
    "        loss, correct = 0.0, 0.0\n",
    "\n",
    "        for data, target in self.test_loader:\n",
    "            if self.device == 'cuda':\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "            output = self.net(data)\n",
    "\n",
    "            loss += self.criterion(output, target).item()\n",
    "\n",
    "            pred = torch.max(output, dim=1)[1]\n",
    "\n",
    "            correct += (pred == target).sum()\n",
    "\n",
    "        loss = float(loss) / len(self.test_loader.dataset)\n",
    "        acc = float(correct) / len(self.test_loader.dataset)\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "        \n",
    "        start_epoch = self.epoch\n",
    "        print(\"Start/Continue training from epoch {}\".format(start_epoch))\n",
    "        \n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            \n",
    "            running_loss, running_acc = 0.0, 0.0\n",
    "            \n",
    "            for idx, (data, target) in enumerate(self.train_loader):\n",
    "                if self.device == 'cuda':\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                data, target = Variable(data), Variable(target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.net(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                for p in list(self.net.parameters()):\n",
    "                    if hasattr(p,'full_precision'):\n",
    "                        p.data.copy_(p.full_precision)\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                for p in list(self.net.parameters()):\n",
    "                    if hasattr(p,'full_precision'):\n",
    "                        p.full_precision.copy_(p.data.clamp_(-1,1))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    running_loss += loss.item()\n",
    "                    pred = torch.max(output, dim=1)[1]\n",
    "                    running_acc += (pred == target).sum()\n",
    "                                        \n",
    "                    if idx % 63 == 0:\n",
    "                        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                            epoch, idx * len(data), len(self.train_loader.dataset),\n",
    "                            100. * idx / len(self.train_loader), loss.item()))\n",
    "                        \n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "            train_loss = float(running_loss) / len(self.train_loader.dataset)\n",
    "            train_acc = float(running_acc) / len(self.train_loader.dataset)\n",
    "            test_loss, test_acc = self.evaluate()\n",
    "\n",
    "            print('\\nTrain set: Average Loss: {:.4f}, Accuracy: {:.0f}%'.format(train_loss, 100. * train_acc))\n",
    "            print('Test set: Average Loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(test_loss, 100. * test_acc))\n",
    "            \n",
    "            self.history.append(epoch)\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            self.test_loss.append(test_loss)\n",
    "            self.test_acc.append(test_acc)\n",
    "            \n",
    "            self.save()\n",
    "            \n",
    "        print(\"Finish training for {} epochs\".format(self.epochs))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start/Continue training from epoch 1\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 8.169657\n",
      "Train Epoch: 1 [6300/60000 (10%)]\tLoss: 4.441452\n",
      "Train Epoch: 1 [12600/60000 (21%)]\tLoss: 9.921505\n",
      "Train Epoch: 1 [18900/60000 (32%)]\tLoss: 7.982539\n",
      "Train Epoch: 1 [25200/60000 (42%)]\tLoss: 7.980196\n",
      "Train Epoch: 1 [31500/60000 (52%)]\tLoss: 1.701658\n",
      "Train Epoch: 1 [37800/60000 (63%)]\tLoss: 8.369499\n",
      "Train Epoch: 1 [44100/60000 (74%)]\tLoss: 5.167326\n",
      "Train Epoch: 1 [50400/60000 (84%)]\tLoss: 5.155433\n",
      "Train Epoch: 1 [56700/60000 (94%)]\tLoss: 4.981294\n",
      "\n",
      "Train set: Average Loss: 0.0566, Accuracy: 89%\n",
      "Test set: Average Loss: 0.0063, Accuracy: 94%\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.700000\n",
      "Train Epoch: 2 [6300/60000 (10%)]\tLoss: 4.500000\n",
      "Train Epoch: 2 [12600/60000 (21%)]\tLoss: 11.248200\n",
      "Train Epoch: 2 [18900/60000 (32%)]\tLoss: 2.980182\n",
      "Train Epoch: 2 [25200/60000 (42%)]\tLoss: 6.329655\n",
      "Train Epoch: 2 [31500/60000 (52%)]\tLoss: 5.580182\n",
      "Train Epoch: 2 [37800/60000 (63%)]\tLoss: 2.380000\n",
      "Train Epoch: 2 [44100/60000 (74%)]\tLoss: 9.520000\n",
      "Train Epoch: 2 [50400/60000 (84%)]\tLoss: 1.320000\n",
      "Train Epoch: 2 [56700/60000 (94%)]\tLoss: 4.100003\n",
      "\n",
      "Train set: Average Loss: 0.0438, Accuracy: 95%\n",
      "Test set: Average Loss: 0.0045, Accuracy: 94%\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 6.380182\n",
      "Train Epoch: 3 [6300/60000 (10%)]\tLoss: 1.160028\n",
      "Train Epoch: 3 [12600/60000 (21%)]\tLoss: 1.820000\n",
      "Train Epoch: 3 [18900/60000 (32%)]\tLoss: 0.727586\n",
      "Train Epoch: 3 [25200/60000 (42%)]\tLoss: 7.808389\n",
      "Train Epoch: 3 [31500/60000 (52%)]\tLoss: 2.447142\n",
      "Train Epoch: 3 [37800/60000 (63%)]\tLoss: 3.240443\n",
      "Train Epoch: 3 [44100/60000 (74%)]\tLoss: 5.180182\n",
      "Train Epoch: 3 [50400/60000 (84%)]\tLoss: 3.507294\n",
      "Train Epoch: 3 [56700/60000 (94%)]\tLoss: 0.520000\n",
      "\n",
      "Train set: Average Loss: 0.0361, Accuracy: 95%\n",
      "Test set: Average Loss: 0.0040, Accuracy: 94%\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 5.181480\n",
      "Train Epoch: 4 [6300/60000 (10%)]\tLoss: 2.580214\n",
      "Train Epoch: 4 [12600/60000 (21%)]\tLoss: 4.180053\n",
      "Train Epoch: 4 [18900/60000 (32%)]\tLoss: 2.380031\n",
      "Train Epoch: 4 [25200/60000 (42%)]\tLoss: 2.721271\n",
      "Train Epoch: 4 [31500/60000 (52%)]\tLoss: 3.782699\n",
      "Train Epoch: 4 [37800/60000 (63%)]\tLoss: 2.284040\n",
      "Train Epoch: 4 [44100/60000 (74%)]\tLoss: 7.460210\n",
      "Train Epoch: 4 [50400/60000 (84%)]\tLoss: 4.021269\n",
      "Train Epoch: 4 [56700/60000 (94%)]\tLoss: 2.947139\n",
      "\n",
      "Train set: Average Loss: 0.0323, Accuracy: 94%\n",
      "Test set: Average Loss: 0.0035, Accuracy: 94%\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 8.081326\n",
      "Train Epoch: 5 [6300/60000 (10%)]\tLoss: 7.680004\n",
      "Train Epoch: 5 [12600/60000 (21%)]\tLoss: 2.068201\n",
      "Train Epoch: 5 [18900/60000 (32%)]\tLoss: 3.620035\n",
      "Train Epoch: 5 [25200/60000 (42%)]\tLoss: 2.181269\n",
      "Train Epoch: 5 [31500/60000 (52%)]\tLoss: 4.148202\n",
      "Train Epoch: 5 [37800/60000 (63%)]\tLoss: 1.740056\n",
      "Train Epoch: 5 [44100/60000 (74%)]\tLoss: 1.882567\n",
      "Train Epoch: 5 [50400/60000 (84%)]\tLoss: 5.400003\n",
      "Train Epoch: 5 [56700/60000 (94%)]\tLoss: 0.740025\n",
      "\n",
      "Train set: Average Loss: 0.0291, Accuracy: 94%\n",
      "Test set: Average Loss: 0.0031, Accuracy: 93%\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.740210\n",
      "Train Epoch: 6 [6300/60000 (10%)]\tLoss: 5.167295\n",
      "Train Epoch: 6 [12600/60000 (21%)]\tLoss: 4.047009\n",
      "Train Epoch: 6 [18900/60000 (32%)]\tLoss: 2.100029\n",
      "Train Epoch: 6 [25200/60000 (42%)]\tLoss: 2.248276\n",
      "Train Epoch: 6 [31500/60000 (52%)]\tLoss: 3.261269\n",
      "Train Epoch: 6 [37800/60000 (63%)]\tLoss: 1.200392\n",
      "Train Epoch: 6 [44100/60000 (74%)]\tLoss: 2.462720\n",
      "Train Epoch: 6 [50400/60000 (84%)]\tLoss: 3.535465\n",
      "Train Epoch: 6 [56700/60000 (94%)]\tLoss: 1.583812\n",
      "\n",
      "Train set: Average Loss: 0.0281, Accuracy: 94%\n",
      "Test set: Average Loss: 0.0030, Accuracy: 93%\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 4.060000\n",
      "Train Epoch: 7 [6300/60000 (10%)]\tLoss: 5.595132\n",
      "Train Epoch: 7 [12600/60000 (21%)]\tLoss: 3.886960\n",
      "Train Epoch: 7 [18900/60000 (32%)]\tLoss: 3.340000\n",
      "Train Epoch: 7 [25200/60000 (42%)]\tLoss: 5.480004\n",
      "Train Epoch: 7 [31500/60000 (52%)]\tLoss: 1.622905\n",
      "Train Epoch: 7 [37800/60000 (63%)]\tLoss: 2.822599\n",
      "Train Epoch: 7 [44100/60000 (74%)]\tLoss: 4.166960\n",
      "Train Epoch: 7 [50400/60000 (84%)]\tLoss: 3.422748\n",
      "Train Epoch: 7 [56700/60000 (94%)]\tLoss: 2.780001\n",
      "\n",
      "Train set: Average Loss: 0.0267, Accuracy: 94%\n",
      "Test set: Average Loss: 0.0028, Accuracy: 93%\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 5.680185\n",
      "Train Epoch: 8 [6300/60000 (10%)]\tLoss: 1.527297\n",
      "Train Epoch: 8 [12600/60000 (21%)]\tLoss: 4.120182\n",
      "Train Epoch: 8 [18900/60000 (32%)]\tLoss: 2.482780\n",
      "Train Epoch: 8 [25200/60000 (42%)]\tLoss: 2.722723\n",
      "Train Epoch: 8 [31500/60000 (52%)]\tLoss: 0.961301\n",
      "Train Epoch: 8 [37800/60000 (63%)]\tLoss: 1.221294\n",
      "Train Epoch: 8 [44100/60000 (74%)]\tLoss: 1.160391\n",
      "Train Epoch: 8 [50400/60000 (84%)]\tLoss: 2.748204\n",
      "Train Epoch: 8 [56700/60000 (94%)]\tLoss: 1.900211\n",
      "\n",
      "Train set: Average Loss: 0.0264, Accuracy: 93%\n",
      "Test set: Average Loss: 0.0029, Accuracy: 93%\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.907298\n",
      "Train Epoch: 9 [6300/60000 (10%)]\tLoss: 4.821685\n",
      "Train Epoch: 9 [12600/60000 (21%)]\tLoss: 3.393980\n",
      "Train Epoch: 9 [18900/60000 (32%)]\tLoss: 3.629662\n",
      "Train Epoch: 9 [25200/60000 (42%)]\tLoss: 5.707319\n",
      "Train Epoch: 9 [31500/60000 (52%)]\tLoss: 2.788383\n",
      "Train Epoch: 9 [37800/60000 (63%)]\tLoss: 1.921843\n",
      "Train Epoch: 9 [44100/60000 (74%)]\tLoss: 1.315160\n",
      "Train Epoch: 9 [50400/60000 (84%)]\tLoss: 1.941485\n",
      "Train Epoch: 9 [56700/60000 (94%)]\tLoss: 3.588226\n",
      "\n",
      "Train set: Average Loss: 0.0254, Accuracy: 93%\n",
      "Test set: Average Loss: 0.0029, Accuracy: 93%\n",
      "\n",
      "Finish training for 10 epochs\n"
     ]
    }
   ],
   "source": [
    "bnn = BNN(name=\"MNIST\", output_dir=\"mnist_logs\")\n",
    "bnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1 weights (min, max): -1.0 1.0\n",
      "FC2 weights (min, max): -1.0 1.0\n",
      "FC3 weights (min, max): -1.0 1.0\n",
      "FC4 weights (min, max): -1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"FC1 weights (min, max):\", torch.min(bnn.net.fc1.weight).item(), torch.max(bnn.net.fc1.weight).item())\n",
    "print(\"FC2 weights (min, max):\", torch.min(bnn.net.fc2.weight).item(), torch.max(bnn.net.fc2.weight).item())\n",
    "print(\"FC3 weights (min, max):\", torch.min(bnn.net.fc3.weight).item(), torch.max(bnn.net.fc3.weight).item())\n",
    "print(\"FC4 weights (min, max):\", torch.min(bnn.net.fc4.weight).item(), torch.max(bnn.net.fc4.weight).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9218,\n",
       " 0.9393,\n",
       " 0.9428,\n",
       " 0.9374,\n",
       " 0.9358,\n",
       " 0.9327,\n",
       " 0.9338,\n",
       " 0.9326,\n",
       " 0.9296,\n",
       " 0.9287]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn.test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
